{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "a=[[np.nan,3,4,5,7],[1,2,3,4,0.1],[2,2,3,0,0.3],[np.nan,2,3,4,5],[2,3,4,1,0.25],[1,4,5,6,0.7]]\n",
    "a=pd.DataFrame(a,columns=['a','b','c','d','rate'])\n",
    "print(a)\n",
    "ix=a.index\n",
    "column_select=['b','d','c']\n",
    "b=a[column_select]\n",
    "print(b)\n",
    "m=0\n",
    "c=a[a==1]if m==0 else a[a==2]\n",
    "print (c)\n",
    "# b=a[a[\"rate\"]<0.3]\n",
    "# a=a[~a[\"d\"].isin([4])]\n",
    "# ix_2=b.index\n",
    "# print(b)\n",
    "# print(a)\n",
    "# print(ix_2[0])\n",
    "# print(ix[-1])\n",
    "# c=a.iloc[ix_2[0]:ix[-1]+1,:]\n",
    "\n",
    "# print(a)\n",
    "# mm=np.min(a['b'].values)\n",
    "# nn=np.max(a['b'].values)\n",
    "# a['b']= a['b'].apply(lambda x: (x - mm) / (nn-mm))  \n",
    "# print(a)\n",
    "\n",
    "# b=a.drop_duplicates([\"b\"],keep=\"last\")\n",
    "# print(b)\n",
    "# ix_b=b.index\n",
    "# c=a.iloc[0:ix_b[0]+1,:]\n",
    "# print(c)\n",
    "\n",
    "# m=list(set(a['c']).intersection(set(a['d'])))\n",
    "# print(m)\n",
    "# test_data=a[a[\"c\"].isin(m)]\n",
    "# print(test_data)\n",
    "# test_data.drop_duplicates([\"c\"],keep=\"last\")\n",
    "# tmp=a[~(a['a']==1)]\n",
    "# print(tmp)\n",
    "# ix=tmp.index\n",
    "\n",
    "# new_data=a.iloc[ix[0]:ix[-1]+1,:]\n",
    "# print(new_data)\n",
    "# new_data['a']=new_data['a'].astype(float).interpolate(method='nearest')\n",
    "# print(new_data)\n",
    "# m=b.loc[[2]]\n",
    "# print(type(m))\n",
    "# print(b.index)\n",
    "# for i in b.index:\n",
    "#     print(i) \n",
    "#     print(b.loc[[i]]['a'].values[0])\n",
    "\n",
    "# print(m[['a','b']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a  b  c  d  rate\n",
      "0  NaN  3  4  5  7.00\n",
      "1  1.0  2  3  4  0.10\n",
      "2  2.0  2  3  0  0.30\n",
      "3  NaN  2  3  4  5.00\n",
      "4  2.0  3  4  1  0.25\n",
      "5  1.0  4  5  6  0.70\n",
      "     c         d      rate\n",
      "0  0.5  0.833333  1.000000\n",
      "2  0.0  0.000000  0.007407\n",
      "3  0.0  0.666667  0.703704\n",
      "4  0.5  0.166667  0.000000\n",
      "5  1.0  1.000000  0.066667\n",
      "     c         d      rate    a  b\n",
      "0  0.5  0.833333  1.000000  NaN  3\n",
      "2  0.0  0.000000  0.007407  2.0  2\n",
      "3  0.0  0.666667  0.703704  NaN  2\n",
      "4  0.5  0.166667  0.000000  2.0  3\n",
      "5  1.0  1.000000  0.066667  1.0  4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "a=[[np.nan,3,4,5,7],[1,2,3,4,0.1],[2,2,3,0,0.3],[np.nan,2,3,4,5],[2,3,4,1,0.25],[1,4,5,6,0.7]]\n",
    "a=pd.DataFrame(a,columns=['a','b','c','d','rate'])\n",
    "print(a)\n",
    "a=a[a.rate>0.1]\n",
    "ix=a.index\n",
    "# print(a)\n",
    "# xx=a[(a.b==3)|((a.c==3)&(a.d==4))]\n",
    "# print(xx)\n",
    "b=a[['a','b']]\n",
    "a=a.drop(['a','b'],axis=1) \n",
    "columns=a.columns.values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "a= min_max_scaler.fit_transform(a)\n",
    "a=pd.DataFrame(a,columns=columns,index=ix)\n",
    "print(a)\n",
    "c=pd.concat([a,b],axis=1)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gcforest'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-1dd754a34543>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgcforest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgcforest\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGCForest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gcforest'"
     ]
    }
   ],
   "source": [
    "from gcforest.gcforest import GCForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-1be439670c5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0my_tain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_tain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mclo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpass_feature_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\pyfile\\lib\\site-packages\\sklearn\\feature_selection\\base.py\u001b[0m in \u001b[0;36mget_support\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0mare\u001b[0m \u001b[0mindices\u001b[0m \u001b[0minto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \"\"\"\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_support_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\pyfile\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py\u001b[0m in \u001b[0;36m_get_support_mask\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_clean_nans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\pyfile\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py\u001b[0m in \u001b[0;36m_clean_nans\u001b[1;34m(scores)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# XXX where should this function be called? fit? scoring functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# themselves?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_float_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\pyfile\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mas_float_array\u001b[1;34m(X, copy, force_all_finite)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mreturn_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'map'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2,SelectFdr,SelectFpr,f_classif\n",
    "from sklearn.metrics.scorer import make_scorer,f1_score,recall_score,precision_score\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import pearsonr\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from sklearn.model_selection import train_test_split,ShuffleSplit,StratifiedKFold,cross_val_score\n",
    "iris = load_iris()\n",
    "X_train = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y_tain = pd.DataFrame(iris.target,columns=['label'])\n",
    "selector=SelectKBest(lambda X,Y:array(map(lambda X:pearsonr(X,Y),X)).T,k=2).fit(X_train,y_tain)\n",
    "mask=selector.get_support()\n",
    "clo=[]\n",
    "for bool, feature in zip(mask, pass_feature_names):\n",
    "        if bool:\n",
    "            clo.append(feature)\n",
    "#     return new_features_pass\n",
    "# clo=list(X_train.columns[selector.get_support(indices=True)])\n",
    "print(iris.feature_names)\n",
    "print(clo)\n",
    "# new_data=pd.concat([X,y],axis=1)\n",
    "# one_train=new_data[new_data.label==0].head(10)\n",
    "# two_train=new_data[new_data.label==1].head(30)\n",
    "# train_data=pd.concat([one_train,two_train],axis=0,ignore_index=True)\n",
    "# from sklearn.utils import shuffle\n",
    "# train_data=shuffle(train_data)\n",
    "# y_data=train_data['label']\n",
    "# x_data=train_data.drop(['label'],axis=1)\n",
    "\n",
    "# x_train,x_test,y_train,y_test=train_test_split(x_data,y_data,test_size=0.3,random_state=5)\n",
    "# print(y_train)\n",
    "# print(x_test)\n",
    "\n",
    "\n",
    "# boston = load_boston()\n",
    "# X = boston[\"data\"]\n",
    "# y = boston[\"target\"]\n",
    "# names = iris.feature_names\n",
    "# RF0=RandomForestClassifier(oob_score=True,criterion='entropy',class_weight='balanced',random_state=10)\n",
    "# selector = SelectFpr(chi2, alpha=0.001)\n",
    "# selector.fit(X, y)\n",
    "# print(X.columns)\n",
    "# print(selector.pvalues_)\n",
    "# print(list(X.columns[selector.get_support(indices=True)]))\n",
    "# X=selector.transform(X)\n",
    "\n",
    "# The list of your K best features\n",
    "# new_features_pass= list(X.columns[selector.get_support(indices=True)])\n",
    "# print(new_features_pass)\n",
    "\n",
    "# rf_pass=RF0.fit(X,y )\n",
    "# print(RF0.feature_importances_)         \n",
    "\n",
    "#             feature_num=0\n",
    "            \n",
    "#             for key in new_features_pass:\n",
    "#                 feature_importances_pass[key].append(RF1.feature_importances_[feature_num])\n",
    "#                 feature_num+=1\n",
    "# selector = SelectFpr(f_classif, alpha=0.05)\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# X_new = selector.transform(X)\n",
    "# print(X_new.shape)\n",
    "\n",
    "# print(selector.get_support())\n",
    "\n",
    " \n",
    "# # 1st way to get the list\n",
    "# vector_names = list(names[selector.get_support(indices=True)])\n",
    "# # vector_names = list(X.columns[selector.get_support(indices=True)])\n",
    "# print(vector_names)\n",
    "# scores=[]\n",
    "# from sklearn.metrics import fbeta_score, make_scorer\n",
    "# from sklearn.metrics.scorer import make_scorer,f1_score,recall_score,precision_score\n",
    "# ftwo_score = make_scorer(f1_score, )\n",
    "# # kfold_5 = cross_validation.KFold(n = len(X), shuffle = True, n_folds = numFolds)\n",
    "# for i in range(X.shape[1]):\n",
    "#     print(X.iloc[:, i:i+1].head(5))\n",
    "#     score = cross_val_score(RF0, X.iloc[:, i:i+1], y.values.ravel(), scoring='f1_weighted',  # 注意X[:, i]和X[:, i:i+1]的区别\n",
    "#                             cv=5)\n",
    "#     scores.append((format(np.mean(score), '.3f'), names[i]))\n",
    "# a=sorted(scores, reverse=True)\n",
    "# xx=[]\n",
    "# for info in a  :\n",
    "#     print(type(info[0]))\n",
    "#     xx.append(info[1])\n",
    "    \n",
    "# print(a)\n",
    "# print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name  age name   id\n",
      "0   张三   18   张三   18\n",
      "1   李四   19   李四  260\n",
      "2   王五   20   王五  280\n",
      "3   张三   18   张三  300\n",
      "  name age   id\n",
      "0   张三  18   18\n",
      "1   李四  19  260\n",
      "2   王五  20  280\n",
      "3   张三  18  300\n",
      "  name name\n",
      "0   张三   张三\n",
      "1   李四   李四\n",
      "2   王五   王五\n",
      "3   张三   张三\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame({'name':['张三','李四','王五','张三'],'age':[18,19,20,18]})\n",
    "df2 = pd.DataFrame({'name':['张三','李四','王五','张三'],'id':[18,260,280,300]})\n",
    "df3 = pd.concat([df1,df2],axis=1) #concat无how\n",
    "print(df3)\n",
    "df4 = df3.T.drop_duplicates().T\n",
    "print(df4)\n",
    "name=['age','id']\n",
    "df5=df3.drop(name,axis=1)\n",
    "print(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#new\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "class Adaboost(object):\n",
    "    def __init__(self, n_learners=20,limit=20):\n",
    "        self.n_learners = n_learners  # 弱学习器的最大个数\n",
    "#         self.fdr=0\n",
    "        self.recall=0.8\n",
    "        self.fdr=0.1\n",
    "        self.limit=limit\n",
    "        self.strong_classifier = []\n",
    "        self.classifierNum     = 0\n",
    "    def count(self):\n",
    "        self.limit+=1\n",
    "    @staticmethod\n",
    "    def cal_gm(features, feature, value, ineq):\n",
    "        \"\"\"\n",
    "        以决策树桩作为基学习器,该基学习器,会将指定特征值<=value的分为-1类,>value的分为1,返回分类结果\n",
    "        本例中base_learner表示基学习器,gm代表该学习器的分类结果\n",
    "        数据集为np.array\n",
    "        :param features: 特征集m*n,连续型数据\n",
    "        :param feature: 给定的特征索引\n",
    "        :param value: 给定的特征值\n",
    "        :param ineq: 不等号的方向,'lt' 等于 '<=', 'gt' 等于 '>'\n",
    "        :return: 返回以feature为最优特征,value为最优特征值的 决策树桩的 分类结果m*1\n",
    "        \"\"\"\n",
    "        gm = np.ones((features.shape[0], 1))\n",
    "#         print(feature)\n",
    "#         print(features[str(feature)])\n",
    "        if ineq == 'lt':  # 把小于等于特征值的样本分为-1\n",
    "            gm[features[feature] <= value, 0] = -1\n",
    "        else:  # 把大于特征值的样本分为-1\n",
    "            gm[features[feature] > value, 0] = -1\n",
    "        return gm\n",
    " \n",
    "  \n",
    "    def cal_cascade(self, features, target):\n",
    "        cur_fpr = 1.0\n",
    "        learner_model=[]\n",
    "        learner_model_arry=[]\n",
    "        while(cur_fpr >= self.fdr):\n",
    "\n",
    "                if cur_fpr < self.fdr:\n",
    "                    break\n",
    "                else:\n",
    "                   \n",
    "                        self.strong_classifier.append (self.training(features, target))\n",
    "#                         print(\"self.strong_classifier[-1]===\",self.strong_classifier[-1])\n",
    "                        [learner_model,fpr,ind]=self.strong_classifier[-1]\n",
    "                        for item in learner_model:\n",
    "                            \n",
    "                            learner_model_arry.append(item)\n",
    "#                         fpr,ind = self.strong_classifier.last[1],self.strong_classifier[i][2]\n",
    "                        print(\"fpr===\",fpr)\n",
    "                        cur_fpr *= fpr\n",
    "\n",
    "                        fp_num = fpr * np.count_nonzero(target == 1 )\n",
    "\n",
    "                        \n",
    "                        features, target = self.updateTrainingDate(features, target, ind)\n",
    "                        \n",
    "                self.classifierNum += 1\n",
    "#         print(self.classifierNum)\n",
    "        return learner_model_arry\n",
    "    \n",
    "    def updateTrainingDate(self,features, target, ind):\n",
    "        if len(ind)!=0:\n",
    "                target=target[ind]\n",
    "                features=features.loc[ind]\n",
    "        return features,target\n",
    "    @staticmethod   \n",
    "    def cal_error(gm, target, weight):\n",
    "        \"\"\"\n",
    "        计算基学习器base_learner的分类误差率\n",
    "        :param gm: base_learner的分类结果m*1\n",
    "        :param target: 标签集m*1\n",
    "        :param weight: 训练集的样本权重m*1\n",
    "        :return: base_learner的分类误差率\n",
    "        \"\"\"\n",
    "       \n",
    "        target_new = np.array(target).reshape(target.shape[0], 1)\n",
    "#         print(target)\n",
    "#         print(gm)\n",
    "        temp = np.multiply(gm, target_new)  # 结果为1则预测正确,结果为-1则预测错误\n",
    "        temp[temp[:, 0] == 1, 0] = 0  # 预测正确的设为0\n",
    "        temp[temp[:, 0] == -1, 0] = 1  # 预测错误的设为1\n",
    "        return np.dot(weight.T, temp)[0, 0]  # base_learner的分类误差率\n",
    "        \n",
    "    @staticmethod\n",
    "    def cal_alpha(error):\n",
    "        \"\"\"计算alpha, 分母的处理是预防error=0的情况\"\"\"\n",
    "        print(\"error\",error)\n",
    "        return np.log((1 - error) / max(error, 1e-16)) / 2\n",
    " \n",
    "    @staticmethod\n",
    "    def update_weight(weight, alpha, target, gm):\n",
    "        \"\"\"\n",
    "        更新下一轮迭代的样本权值并返回\n",
    "        :param weight: 待更新的样本权值D,m*1\n",
    "        :param alpha: 基学习器的系数\n",
    "        :param target: 标签集m*1\n",
    "        :param gm: base_learner的分类结果m*1\n",
    "        :return: 用于下一轮迭代的样本权值\n",
    "        \"\"\"\n",
    "        target_new = np.array(target).reshape(target.shape[0], 1)\n",
    "        next_weight = np.multiply(weight, np.exp(-alpha * np.multiply(target_new, gm)))\n",
    "        return next_weight / np.sum(next_weight)\n",
    " \n",
    "    def create_base_learner(self, features, target, weight):\n",
    "        \"\"\"\n",
    "        创建误分类率最小的基学习器\n",
    "        :param features: 特征集m*n\n",
    "        :param target: 标签集m*1\n",
    "        :param weight: 样本权值\n",
    "        :return: 分类误差率最小的基学习器和训练时的预测结果\n",
    "        \"\"\"\n",
    "        m, n = features.shape\n",
    "        num_steps = 100  # 定义特征值取值的个数\n",
    "        base_learner = {}\n",
    "        min_error, best_feature, best_value, best_ineq = np.inf, -1, 0, 'lt'\n",
    "        bestgm = np.zeros((m, 1))\n",
    "        for feature in features.columns.values:\n",
    "#             print(\"feature==\",feature)\n",
    "            range_min = features[feature].min()\n",
    "            range_max = features[feature].max()\n",
    "            step = (range_max - range_min) / num_steps  # 根据该特征下特征值的最大最小值,设定步长\n",
    "            for i in range(num_steps + 1):  # 根据步长选取10个值\n",
    "                value = range_min + float(i) * step\n",
    "                for ineq in ('lt', 'gt'):\n",
    "                    gm = self.cal_gm(features, feature, value, ineq)  # 预测结果\n",
    "                    error = self.cal_error(gm, target, weight)  # 误差分类率\n",
    "                    if error < min_error:  # 更新最小误差分类率\n",
    "                        min_error = error\n",
    "                        best_feature =feature\n",
    "                        best_value = value\n",
    "                        best_ineq = ineq\n",
    "                        bestgm = gm\n",
    "        alpha = self.cal_alpha(min_error)  # 根据最小误差,计算alpha\n",
    "        base_learner['Feature'] = best_feature  # 保存决策树桩的信息\n",
    "        base_learner['Value'] = best_value\n",
    "        base_learner['Alpha'] = alpha\n",
    "        base_learner['ineq'] = best_ineq\n",
    "        return base_learner, bestgm\n",
    " \n",
    "    def training(self, features, target):\n",
    "        \"\"\"\n",
    "        根据Adaboost算法,得出最终的线性加法模型,\n",
    "        :param features: 特征集m*n\n",
    "        :param target: 标签集m*1\n",
    "        :return: 加法模型,基学习器组成的列表\n",
    "        \"\"\"\n",
    "#         features = np.array(features)\n",
    "        P=features\n",
    "        \n",
    "#         target = np.array(target).reshape(features.shape[0], 1)# 可把一维数组转化成二维的m*1\n",
    "        m = features.shape[0]#多少行\n",
    "        negNum=np.count_nonzero(target==1)\n",
    "        posNum=np.count_nonzero(target==-1)\n",
    "        weight = [1.0/(2*negNum) if i==1 else 1.0/(2*posNum)  for i in target]\n",
    "        weight=np.array(weight).reshape(len(weight), 1)\n",
    "#         weight = np.ones((m, 1)) / m  # 初始化样本权值\n",
    "#         print(weight)\n",
    "        learner_arr = []  # 加法模型,存储基学习器的列表\n",
    "        fx = np.zeros((m, 1))  # 预测值\n",
    "#         print(fx)\n",
    "        flag=1\n",
    "        alpha_t=[]\n",
    "        alpha_ht_sum=np.zero((len(weight),))\n",
    "        for i in range(self.n_learners):\n",
    "            print(\"target\",target.shape[0])\n",
    "            base_learner, gm = self.create_base_learner(features, target, weight)\n",
    "            \n",
    "            fx += base_learner['Alpha'] * gm\n",
    "            alpha_t.append(base_learner['Alpha'])\n",
    "            rhs_thresh = 0.5 * sum(alpha_t)\n",
    "            predicton = [1 if x >= rhs_thresh else -1 for x in fx]\n",
    "            corrct = [1 if a == b else 0 for a, b in zip(predicton, target)]\n",
    "            recall_weight=[]\n",
    "            ind=[]          \n",
    "#             print(\"target\",list(target))\n",
    "            for a,b,c in zip(predicton,target,target.index):\n",
    "                if a==-1 and b==-1:\n",
    "                    recall_weight.append(1)\n",
    "                    ind.append(c)\n",
    "                if a==1 and b==-1:\n",
    "                    recall_weight.append(0)\n",
    "                    ind.append(c)\n",
    "                if a==1 and b==1:\n",
    "                    recall_weight.append(-1)\n",
    "                    \n",
    "            recall_i=recall_weight.count(1)/(recall_weight.count(1)+recall_weight.count(0))\n",
    "#             print(\"recall_weight\",recall_weight)\n",
    "            print('recal_id',recall_i)\n",
    "            if ((recall_weight.count(0)+recall_weight.count(-1))==0):\n",
    "                f_layer=1\n",
    "            else:\n",
    "                #0-0,0-1\n",
    "                f_layer=recall_weight.count(0)/(recall_weight.count(0)+recall_weight.count(-1))\n",
    "#                 f_layer=corrct.count(0)/len(corrct)\n",
    "               \n",
    "            \n",
    "            weight = self.update_weight(weight, base_learner['Alpha'], target, gm) \n",
    "            while(recall_i < self.recall):\n",
    "                print(base_learner['Feature'])\n",
    "                \n",
    "                recall_weight=[]\n",
    "                ind=[]\n",
    "#                 base_learner, gm = self.create_base_learner(features, target, weight)\n",
    "                tmp_value=base_learner['Value']\n",
    "#                 print(\"base_learner['Value']\",tmp_value)\n",
    "                rhs_thresh_new-=abs(rhs_thresh)/100\n",
    "#                 print(\"base_learner['value']===\",base_learner['Value'])\n",
    "#                 gm = self.cal_gm(features, base_learner['Feature'], base_learner['Value'], base_learner['ineq'])\n",
    "#                 fx += base_learner['Alpha'] * gm\n",
    "                \n",
    "#                 fx += base_learner['Alpha'] * gm\n",
    "                predicton = [1 if x >= rhs_thresh_new else -1 for x in fx]\n",
    "                corrct = [1 if a == b else 0 for a, b in zip(predicton, target)]\n",
    "                for a,b,c in zip(prediction,target,target.index):\n",
    "                    if a==-1 and b==-1:\n",
    "                        recall_weight.append(1)\n",
    "                        ind.append(c)\n",
    "                    if a==1 and b==-1:\n",
    "                        recall_weight.append(0)\n",
    "                        ind.append(c)\n",
    "                    if a==1 and b==1:\n",
    "                        recall_weight.append(-1)\n",
    "                recall_i=recall_weight.count(1)/(recall_weight.count(1)+recall_weight.count(0))\n",
    "                if ((recall_weight.count(0)+recall_weight.count(-1))==0):\n",
    "                    f_layer=1\n",
    "                else:\n",
    "                #0-0,0-1\n",
    "                    f_layer=recall_weight.count(0)/(recall_weight.count(0)+recall_weight.count(-1))\n",
    "#                 f_layer=corrct.count(0)/len(corrct)\n",
    "               \n",
    "                print(\"recall_i_while:\",recall_i)\n",
    "            \n",
    "            learner_arr.append(base_learner)\n",
    "               \n",
    "            if recall_i >= self.recall and f_layer < 0.1:\n",
    "                print(\"yes\")\n",
    "#                 return True  # 对训练集分类正确率达百分百,则跳出循环\n",
    "                break\n",
    "            if corrct.count(1) / len(corrct) == 1:  # 对训练集分类正确率达百分百,则跳出循环\n",
    "                break\n",
    "#         print(\"ind\",ind)# 如果没达到百分百,那么继续\n",
    "        \n",
    "            \n",
    "        return [learner_arr,f_layer,ind]\n",
    " \n",
    "    def predict(self, learner_arr, features):\n",
    "        \"\"\"\n",
    "        根据训练好的加法模型,对样本X进行分类\n",
    "        :param learner_arr: 训练好的加法模型\n",
    "        :param features: 待分类样本\n",
    "        :return: 分类结果\n",
    "        \"\"\"\n",
    "#         features = np.array(features)\n",
    "        if features.ndim == 1:  # 如果是1维的单个样本,则转换成二维的\n",
    "            features = features.reshape(1, features.shape[0])\n",
    "        fx = np.zeros((features.shape[0], 1))\n",
    "        fx_tmp = np.zeros((features.shape[0], 1))\n",
    "       \n",
    "        prediction=[]\n",
    "        ix=features.index\n",
    "        print(learner_arr)      \n",
    "#         print(features[['pr_test_files','commits']])\n",
    "#         rhs_thresh = 0.5 * sum(alpha_t)\n",
    "#         alphat_ht = alpha_t[t] * ht\n",
    "#         alphat_ht_sum = alphat_ht_sum + alphat_ht\n",
    "        for number in features.index:\n",
    "            flag=0\n",
    "            fx=[]\n",
    "#             base_learner=learner_arr[-1]\n",
    "            for base_learner in learner_arr:\n",
    "#                 print(\"base_learner===\",base_learner)\n",
    "\n",
    "                    gm = self.cal_gm(features.loc[[number]], base_learner['Feature'], base_learner['Value'], base_learner['ineq'])\n",
    "        #             print(\"gm====\",gm)\n",
    "                    fx_tmp=base_learner['Alpha'] * gm\n",
    "                    print(fx_tmp)\n",
    "                    if fx_tmp[0]<0:\n",
    "        #                     print(fx_tmp[0])\n",
    "                        prediction.append(-1)\n",
    "                        flag=1\n",
    "                        break\n",
    "                    else:\n",
    "                        fx += fx_tmp\n",
    "#                         prediction.append(1)\n",
    "#                 for x in fx_tmp:\n",
    "#                     if x<0:\n",
    "#                         prediction.append(-1)\n",
    "#                     else:\n",
    "#                         fx += fx_tmp\n",
    "#             if flag==0:\n",
    "#                 for x in fx:\n",
    "#                     print(fx)\n",
    "#                     prediction.append(1) \n",
    "#     if x>=0 else  prediction.append(-1) \n",
    "        return prediction\n",
    "\n",
    "    \n",
    "def test():\n",
    "    from sklearn.datasets import load_iris\n",
    "    import math\n",
    "    from collections import Counter\n",
    "    from sklearn.model_selection import train_test_split,ShuffleSplit,StratifiedKFold,cross_val_score\n",
    "\n",
    "    new_data=pd.read_csv('./structr/new_metric/gh_team_size/DSpace@DSpace_newmerge.csv',low_memory=False)\n",
    "    b=new_data[['pr_status','last_label','now_label','id','now_build_id']]\n",
    "    new_data=new_data.drop(['pr_status','last_label','now_label','id','now_build_id'],axis=1)\n",
    "    ix=new_data.index\n",
    "#             new_data=new_data.drop(['import_gum','import_change_count'],axis=1)\n",
    "    feature_names = list(new_data.columns.values)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a= min_max_scaler.fit_transform(new_data)\n",
    "    aa=pd.DataFrame(a,columns=feature_names,index=ix)\n",
    "    new_data=pd.concat([aa,b],axis=1)\n",
    "    new_data=new_data.drop(['now_duration'],axis=1)\n",
    "    new_data_fail=new_data[~new_data['last_label'].isin([\"1\"])]\n",
    "    test_size=math.ceil(new_data_fail.shape[0]/5)\n",
    "    \n",
    "    new_data_ix=new_data.index\n",
    "    test_data_begin=new_data_fail.tail(test_size)\n",
    "   \n",
    "    ix=test_data_begin.index\n",
    "   \n",
    "\n",
    "    test_data=new_data.iloc[ix[0]:new_data_ix[-1]+1,:]\n",
    "  \n",
    "    ix_test=test_data.index\n",
    "    train_data=new_data.iloc[0:ix_test[0],:]\n",
    "    train_data=train_data.drop(['id','now_build_id'],axis=1)\n",
    "    test_data=test_data.drop(['id','now_build_id'],axis=1)\n",
    "    y_train=train_data['now_label']\n",
    "    \n",
    "    x_train=train_data.drop(['now_label'],axis=1)\n",
    "    y_test=test_data['now_label']  \n",
    "    x_test=test_data.drop(['now_label'],axis=1)\n",
    "    \n",
    "    y_train[y_train[:] == 0] = -1\n",
    "    y_test[y_test[:] == 0]=-1\n",
    "    \n",
    "#     target_new = np.array(y_train).reshape(y_train.shape[0], 1)\n",
    "    ada = Adaboost(n_learners=30,limit=5)\n",
    "    print(y_train.shape[0])\n",
    "    learner_arr = ada.cal_cascade(x_train, y_train)\n",
    "    predicton = ada.predict(learner_arr, x_train)\n",
    "#     iForest= IsolationForest(n_estimators=20,contamination=0.25)\n",
    "\n",
    "#     iForest=iForest.fit(x_train)\n",
    "\n",
    "#     predicton = iForest.predict(x_test)\n",
    "\n",
    "    \n",
    "    from sklearn.metrics.scorer import make_scorer,f1_score,recall_score,precision_score\n",
    "    print(len(predicton))\n",
    "    print(( y_train))\n",
    "    correct = [1 if a == b else 0 for a, b in zip(predicton, y_train)]\n",
    "    print(f1_score( y_train,predicton,average=None)[0])\n",
    "    print(f1_score( y_train,predicton,average='weighted'))\n",
    "    print(correct.count(1) / len(correct))\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pyfile\\lib\\site-packages\\ipykernel_launcher.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\pyfile\\lib\\site-packages\\ipykernel_launcher.py:363: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501\n",
      "target 1501\n",
      "error 0.22217481849998558\n",
      "recal_id 0.577922077922078\n",
      "now_duration\n",
      "base_learner['value']=== 0.04047436787996666\n",
      "recall_i_while: 0.577922077922078\n",
      "now_duration\n",
      "base_learner['value']=== 0.04087911155876633\n",
      "recall_i_while: 0.577922077922078\n",
      "now_duration\n",
      "base_learner['value']=== 0.04128790267435399\n",
      "recall_i_while: 0.5844155844155844\n",
      "now_duration\n",
      "base_learner['value']=== 0.04170078170109753\n",
      "recall_i_while: 0.5844155844155844\n",
      "now_duration\n",
      "base_learner['value']=== 0.04211778951810851\n",
      "recall_i_while: 0.5844155844155844\n",
      "now_duration\n",
      "base_learner['value']=== 0.042538967413289594\n",
      "recall_i_while: 0.5844155844155844\n",
      "now_duration\n",
      "base_learner['value']=== 0.04296435708742249\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.04339400065829671\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.043827940664879675\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.04426622007152847\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.04470888227224376\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.0451559710949662\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.04560753080591586\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.046063606113975024\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.04652424217511478\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.04698948459686592\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.047459379442834584\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.04793397323726293\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.04841331296963556\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.048897446099331915\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.04938642056032523\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.049880284765928484\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.05037908761358777\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.05088287848972364\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.051391707274620876\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.051905624347367084\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.05242468059084075\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.05294892739674916\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.05347841667071665\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.054013200837423815\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.05455333284579805\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.055098866174256036\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.055649854835998595\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.05620635338435858\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.05676841691820217\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.05733610108738419\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.057909462098258035\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.058488556719240616\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.05907344228643302\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.05966417670929735\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.060260818476390324\n",
      "recall_i_while: 0.5909090909090909\n",
      "now_duration\n",
      "base_learner['value']=== 0.06086342666115423\n",
      "recall_i_while: 0.5974025974025974\n",
      "now_duration\n",
      "base_learner['value']=== 0.06147206092776577\n",
      "recall_i_while: 0.5974025974025974\n",
      "now_duration\n",
      "base_learner['value']=== 0.062086781537043424\n",
      "recall_i_while: 0.5974025974025974\n",
      "now_duration\n",
      "base_learner['value']=== 0.06270764935241385\n",
      "recall_i_while: 0.5974025974025974\n",
      "now_duration\n",
      "base_learner['value']=== 0.063334725845938\n",
      "recall_i_while: 0.6038961038961039\n",
      "now_duration\n",
      "base_learner['value']=== 0.06396807310439738\n",
      "recall_i_while: 0.6103896103896104\n",
      "now_duration\n",
      "base_learner['value']=== 0.06460775383544136\n",
      "recall_i_while: 0.6103896103896104\n",
      "now_duration\n",
      "base_learner['value']=== 0.06525383137379577\n",
      "recall_i_while: 0.6103896103896104\n",
      "now_duration\n",
      "base_learner['value']=== 0.06590636968753373\n",
      "recall_i_while: 0.6233766233766234\n",
      "now_duration\n",
      "base_learner['value']=== 0.06656543338440907\n",
      "recall_i_while: 0.6233766233766234\n",
      "now_duration\n",
      "base_learner['value']=== 0.06723108771825316\n",
      "recall_i_while: 0.6233766233766234\n",
      "now_duration\n",
      "base_learner['value']=== 0.06790339859543569\n",
      "recall_i_while: 0.6428571428571429\n",
      "now_duration\n",
      "base_learner['value']=== 0.06858243258139005\n",
      "recall_i_while: 0.6428571428571429\n",
      "now_duration\n",
      "base_learner['value']=== 0.06926825690720395\n",
      "recall_i_while: 0.6558441558441559\n",
      "now_duration\n",
      "base_learner['value']=== 0.06996093947627599\n",
      "recall_i_while: 0.6623376623376623\n",
      "now_duration\n",
      "base_learner['value']=== 0.07066054887103874\n",
      "recall_i_while: 0.6688311688311688\n",
      "now_duration\n",
      "base_learner['value']=== 0.07136715435974914\n",
      "recall_i_while: 0.6688311688311688\n",
      "now_duration\n",
      "base_learner['value']=== 0.07208082590334662\n",
      "recall_i_while: 0.6753246753246753\n",
      "now_duration\n",
      "base_learner['value']=== 0.07280163416238009\n",
      "recall_i_while: 0.6883116883116883\n",
      "now_duration\n",
      "base_learner['value']=== 0.07352965050400388\n",
      "recall_i_while: 0.6883116883116883\n",
      "now_duration\n",
      "base_learner['value']=== 0.07426494700904392\n",
      "recall_i_while: 0.6883116883116883\n",
      "now_duration\n",
      "base_learner['value']=== 0.07500759647913435\n",
      "recall_i_while: 0.6883116883116883\n",
      "now_duration\n",
      "base_learner['value']=== 0.0757576724439257\n",
      "recall_i_while: 0.7077922077922078\n",
      "now_duration\n",
      "base_learner['value']=== 0.07651524916836495\n",
      "recall_i_while: 0.7077922077922078\n",
      "now_duration\n",
      "base_learner['value']=== 0.0772804016600486\n",
      "recall_i_while: 0.7077922077922078\n",
      "now_duration\n",
      "base_learner['value']=== 0.07805320567664908\n",
      "recall_i_while: 0.7077922077922078\n",
      "now_duration\n",
      "base_learner['value']=== 0.07883373773341558\n",
      "recall_i_while: 0.7207792207792207\n",
      "now_duration\n",
      "base_learner['value']=== 0.07962207511074973\n",
      "recall_i_while: 0.7207792207792207\n",
      "now_duration\n",
      "base_learner['value']=== 0.08041829586185723\n",
      "recall_i_while: 0.7207792207792207\n",
      "now_duration\n",
      "base_learner['value']=== 0.0812224788204758\n",
      "recall_i_while: 0.7337662337662337\n",
      "now_duration\n",
      "base_learner['value']=== 0.08203470360868055\n",
      "recall_i_while: 0.7402597402597403\n",
      "now_duration\n",
      "base_learner['value']=== 0.08285505064476735\n",
      "recall_i_while: 0.7402597402597403\n",
      "now_duration\n",
      "base_learner['value']=== 0.08368360115121502\n",
      "recall_i_while: 0.7402597402597403\n",
      "now_duration\n",
      "base_learner['value']=== 0.08452043716272717\n",
      "recall_i_while: 0.7662337662337663\n",
      "now_duration\n",
      "base_learner['value']=== 0.08536564153435444\n",
      "recall_i_while: 0.7922077922077922\n",
      "now_duration\n",
      "base_learner['value']=== 0.08621929794969799\n",
      "recall_i_while: 0.7922077922077922\n",
      "now_duration\n",
      "base_learner['value']=== 0.08708149092919497\n",
      "recall_i_while: 0.7987012987012987\n",
      "now_duration\n",
      "base_learner['value']=== 0.08795230583848691\n",
      "recall_i_while: 0.7987012987012987\n",
      "now_duration\n",
      "base_learner['value']=== 0.08883182889687179\n",
      "recall_i_while: 0.7987012987012987\n",
      "now_duration\n",
      "base_learner['value']=== 0.08972014718584051\n",
      "recall_i_while: 0.8246753246753247\n",
      "yes\n",
      "fpr=== 0.04703328509406657\n",
      "[{'Feature': 'now_duration', 'Value': 0.08972014718584051, 'Alpha': 0.6265186268931653, 'ineq': 'lt'}]\n",
      "1501\n",
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "5       1\n",
      "6       1\n",
      "7       1\n",
      "8       1\n",
      "9       1\n",
      "10      1\n",
      "11      1\n",
      "12      1\n",
      "13      1\n",
      "14      1\n",
      "15      1\n",
      "16      1\n",
      "17      1\n",
      "18      1\n",
      "19      1\n",
      "20      1\n",
      "21      1\n",
      "22      1\n",
      "23      1\n",
      "24      1\n",
      "25      1\n",
      "26      1\n",
      "27      1\n",
      "28      1\n",
      "29      1\n",
      "       ..\n",
      "1471   -1\n",
      "1472   -1\n",
      "1473   -1\n",
      "1474   -1\n",
      "1475   -1\n",
      "1476   -1\n",
      "1477   -1\n",
      "1478   -1\n",
      "1479   -1\n",
      "1480   -1\n",
      "1481   -1\n",
      "1482   -1\n",
      "1483   -1\n",
      "1484   -1\n",
      "1485   -1\n",
      "1486   -1\n",
      "1487    1\n",
      "1488    1\n",
      "1489   -1\n",
      "1490   -1\n",
      "1491   -1\n",
      "1492   -1\n",
      "1493   -1\n",
      "1494   -1\n",
      "1495   -1\n",
      "1496   -1\n",
      "1497   -1\n",
      "1498   -1\n",
      "1499   -1\n",
      "1500   -1\n",
      "Name: now_label, Length: 1501, dtype: int64\n",
      "0.26765015806111697\n",
      "0.6210656799584231\n",
      "0.5369753497668222\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#origin\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "class Adaboost(object):\n",
    "    def __init__(self, n_learners=20,limit=20):\n",
    "        self.n_learners = n_learners  # 弱学习器的最大个数\n",
    "#         self.fdr=0\n",
    "        self.recall=0.8\n",
    "        self.fdr=0.1\n",
    "        self.limit=limit\n",
    "        self.strong_classifier = []\n",
    "        self.classifierNum     = 0\n",
    "    def count(self):\n",
    "        self.limit+=1\n",
    "    @staticmethod\n",
    "    def cal_gm(features, feature, value, ineq):\n",
    "        \"\"\"\n",
    "        以决策树桩作为基学习器,该基学习器,会将指定特征值<=value的分为-1类,>value的分为1,返回分类结果\n",
    "        本例中base_learner表示基学习器,gm代表该学习器的分类结果\n",
    "        数据集为np.array\n",
    "        :param features: 特征集m*n,连续型数据\n",
    "        :param feature: 给定的特征索引\n",
    "        :param value: 给定的特征值\n",
    "        :param ineq: 不等号的方向,'lt' 等于 '<=', 'gt' 等于 '>'\n",
    "        :return: 返回以feature为最优特征,value为最优特征值的 决策树桩的 分类结果m*1\n",
    "        \"\"\"\n",
    "        gm = np.ones((features.shape[0], 1))\n",
    "#         print(feature)\n",
    "#         print(features[str(feature)])\n",
    "        if ineq == 'lt':  # 把小于等于特征值的样本分为-1\n",
    "            gm[features[feature] <= value, 0] = -1\n",
    "        else:  # 把大于特征值的样本分为-1\n",
    "            gm[features[feature] > value, 0] = -1\n",
    "        return gm\n",
    " \n",
    "  \n",
    "    def cal_cascade(self, features, target):\n",
    "        cur_fpr = 1.0\n",
    "        learner_model=[]\n",
    "        learner_model_arry=[]\n",
    "        while(cur_fpr >= self.fdr):\n",
    "\n",
    "                if cur_fpr < self.fdr:\n",
    "                    break\n",
    "                else:\n",
    "                   \n",
    "                        self.strong_classifier.append (self.training(features, target))\n",
    "#                         print(\"self.strong_classifier[-1]===\",self.strong_classifier[-1])\n",
    "                        [learner_model,fpr,ind]=self.strong_classifier[-1]\n",
    "                        for item in learner_model:\n",
    "                            \n",
    "                            learner_model_arry.append(item)\n",
    "#                         fpr,ind = self.strong_classifier.last[1],self.strong_classifier[i][2]\n",
    "                        print(\"fpr===\",fpr)\n",
    "                        cur_fpr *= fpr\n",
    "\n",
    "                        fp_num = fpr * np.count_nonzero(target == 1 )\n",
    "\n",
    "                        \n",
    "                        features, target = self.updateTrainingDate(features, target, ind)\n",
    "                        \n",
    "                self.classifierNum += 1\n",
    "#         print(self.classifierNum)\n",
    "        return learner_model_arry\n",
    "    \n",
    "    def updateTrainingDate(self,features, target, ind):\n",
    "        if len(ind)!=0:\n",
    "                target=target[ind]\n",
    "                features=features.loc[ind]\n",
    "        return features,target\n",
    "    @staticmethod   \n",
    "    def cal_error(gm, target, weight):\n",
    "        \"\"\"\n",
    "        计算基学习器base_learner的分类误差率\n",
    "        :param gm: base_learner的分类结果m*1\n",
    "        :param target: 标签集m*1\n",
    "        :param weight: 训练集的样本权重m*1\n",
    "        :return: base_learner的分类误差率\n",
    "        \"\"\"\n",
    "       \n",
    "        target_new = np.array(target).reshape(target.shape[0], 1)\n",
    "#         print(target)\n",
    "#         print(gm)\n",
    "        temp = np.multiply(gm, target_new)  # 结果为1则预测正确,结果为-1则预测错误\n",
    "        temp[temp[:, 0] == 1, 0] = 0  # 预测正确的设为0\n",
    "        temp[temp[:, 0] == -1, 0] = 1  # 预测错误的设为1\n",
    "        return np.dot(weight.T, temp)[0, 0]  # base_learner的分类误差率\n",
    "        \n",
    "    @staticmethod\n",
    "    def cal_alpha(error):\n",
    "        \"\"\"计算alpha, 分母的处理是预防error=0的情况\"\"\"\n",
    "        print(\"error\",error)\n",
    "        return np.log((1 - error) / max(error, 1e-16)) / 2\n",
    " \n",
    "    @staticmethod\n",
    "    def update_weight(weight, alpha, target, gm):\n",
    "        \"\"\"\n",
    "        更新下一轮迭代的样本权值并返回\n",
    "        :param weight: 待更新的样本权值D,m*1\n",
    "        :param alpha: 基学习器的系数\n",
    "        :param target: 标签集m*1\n",
    "        :param gm: base_learner的分类结果m*1\n",
    "        :return: 用于下一轮迭代的样本权值\n",
    "        \"\"\"\n",
    "        target_new = np.array(target).reshape(target.shape[0], 1)\n",
    "        next_weight = np.multiply(weight, np.exp(-alpha * np.multiply(target_new, gm)))\n",
    "        return next_weight / np.sum(next_weight)\n",
    " \n",
    "    def create_base_learner(self, features, target, weight):\n",
    "        \"\"\"\n",
    "        创建误分类率最小的基学习器\n",
    "        :param features: 特征集m*n\n",
    "        :param target: 标签集m*1\n",
    "        :param weight: 样本权值\n",
    "        :return: 分类误差率最小的基学习器和训练时的预测结果\n",
    "        \"\"\"\n",
    "        m, n = features.shape\n",
    "        num_steps = 100  # 定义特征值取值的个数\n",
    "        base_learner = {}\n",
    "        min_error, best_feature, best_value, best_ineq = np.inf, -1, 0, 'lt'\n",
    "        bestgm = np.zeros((m, 1))\n",
    "        for feature in features.columns.values:\n",
    "#             print(\"feature==\",feature)\n",
    "            range_min = features[feature].min()\n",
    "            range_max = features[feature].max()\n",
    "            step = (range_max - range_min) / num_steps  # 根据该特征下特征值的最大最小值,设定步长\n",
    "            for i in range(num_steps + 1):  # 根据步长选取10个值\n",
    "                value = range_min + float(i) * step\n",
    "                for ineq in ('lt', 'gt'):\n",
    "                    gm = self.cal_gm(features, feature, value, ineq)  # 预测结果\n",
    "                    error = self.cal_error(gm, target, weight)  # 误差分类率\n",
    "                    if error < min_error:  # 更新最小误差分类率\n",
    "                        min_error = error\n",
    "                        best_feature =feature\n",
    "                        best_value = value\n",
    "                        best_ineq = ineq\n",
    "                        bestgm = gm\n",
    "        alpha = self.cal_alpha(min_error)  # 根据最小误差,计算alpha\n",
    "        base_learner['Feature'] = best_feature  # 保存决策树桩的信息\n",
    "        base_learner['Value'] = best_value\n",
    "        base_learner['Alpha'] = alpha\n",
    "        base_learner['ineq'] = best_ineq\n",
    "        return base_learner, bestgm\n",
    " \n",
    "    def training(self, features, target):\n",
    "        \"\"\"\n",
    "        根据Adaboost算法,得出最终的线性加法模型,\n",
    "        :param features: 特征集m*n\n",
    "        :param target: 标签集m*1\n",
    "        :return: 加法模型,基学习器组成的列表\n",
    "        \"\"\"\n",
    "#         features = np.array(features)\n",
    "        P=features\n",
    "        \n",
    "#         target = np.array(target).reshape(features.shape[0], 1)# 可把一维数组转化成二维的m*1\n",
    "        m = features.shape[0]#多少行\n",
    "        negNum=np.count_nonzero(target==1)\n",
    "        posNum=np.count_nonzero(target==-1)\n",
    "        weight = [1.0/(2*negNum) if i==1 else 1.0/(2*posNum)  for i in target]\n",
    "        weight=np.array(weight).reshape(len(weight), 1)\n",
    "#         weight = np.ones((m, 1)) / m  # 初始化样本权值\n",
    "#         print(weight)\n",
    "        learner_arr = []  # 加法模型,存储基学习器的列表\n",
    "        fx = np.zeros((m, 1))  # 预测值\n",
    "#         print(fx)\n",
    "        flag=1\n",
    "        for i in range(self.n_learners):\n",
    "            print(\"target\",target.shape[0])\n",
    "            base_learner, gm = self.create_base_learner(features, target, weight)\n",
    "            \n",
    "            fx += base_learner['Alpha'] * gm\n",
    "            predicton = [1 if x >= 0 else -1 for x in fx]\n",
    "            corrct = [1 if a == b else 0 for a, b in zip(predicton, target)]\n",
    "            recall_weight=[]\n",
    "            ind=[]          \n",
    "#             print(\"target\",list(target))\n",
    "            for a,b,c in zip(predicton,target,target.index):\n",
    "                if a==-1 and b==-1:\n",
    "                    recall_weight.append(1)\n",
    "                    ind.append(c)\n",
    "                if a==1 and b==-1:\n",
    "                    recall_weight.append(0)\n",
    "                    ind.append(c)\n",
    "                if a==1 and b==1:\n",
    "                    recall_weight.append(-1)\n",
    "                    \n",
    "            recall_i=recall_weight.count(1)/(recall_weight.count(1)+recall_weight.count(0))\n",
    "#             print(\"recall_weight\",recall_weight)\n",
    "            print('recal_id',recall_i)\n",
    "            if ((recall_weight.count(0)+recall_weight.count(-1))==0):\n",
    "                f_layer=1\n",
    "            else:\n",
    "                #0-0,0-1\n",
    "                f_layer=recall_weight.count(0)/(recall_weight.count(0)+recall_weight.count(-1))\n",
    "#                 f_layer=corrct.count(0)/len(corrct)\n",
    "               \n",
    "            \n",
    "            weight = self.update_weight(weight, base_learner['Alpha'], target, gm) \n",
    "            while(recall_i < self.recall):\n",
    "                print(base_learner['Feature'])\n",
    "                \n",
    "                recall_weight=[]\n",
    "                ind=[]\n",
    "#                 base_learner, gm = self.create_base_learner(features, target, weight)\n",
    "                tmp_value=base_learner['Value']\n",
    "#                 print(\"base_learner['Value']\",tmp_value)\n",
    "                if tmp_value!=0:\n",
    "                    base_learner['Value']+= abs(tmp_value)/100\n",
    "                    flag=1\n",
    "                else:\n",
    "                    flag=0\n",
    "                    break\n",
    "                print(\"base_learner['value']===\",base_learner['Value'])\n",
    "                gm = self.cal_gm(features, base_learner['Feature'], base_learner['Value'], base_learner['ineq'])\n",
    "#                 fx += base_learner['Alpha'] * gm\n",
    "                \n",
    "#                 fx += base_learner['Alpha'] * gm\n",
    "                predicton = [1 if x >= 0 else -1 for x in fx]\n",
    "                corrct = [1 if a == b else 0 for a, b in zip(predicton, target)]\n",
    "                for a,b,c in zip(gm,target,target.index):\n",
    "                    if a==-1 and b==-1:\n",
    "                        recall_weight.append(1)\n",
    "                        ind.append(c)\n",
    "                    if a==1 and b==-1:\n",
    "                        recall_weight.append(0)\n",
    "                        ind.append(c)\n",
    "                    if a==1 and b==1:\n",
    "                        recall_weight.append(-1)\n",
    "                recall_i=recall_weight.count(1)/(recall_weight.count(1)+recall_weight.count(0))\n",
    "                print(\"recall_i_while:\",recall_i)\n",
    "            if flag!=0:\n",
    "                learner_arr.append(base_learner)\n",
    "               \n",
    "            if recall_i >= self.recall and f_layer < 0.1:\n",
    "                print(\"yes\")\n",
    "#                 return True  # 对训练集分类正确率达百分百,则跳出循环\n",
    "                break\n",
    "            if corrct.count(1) / len(corrct) == 1:  # 对训练集分类正确率达百分百,则跳出循环\n",
    "                break\n",
    "#         print(\"ind\",ind)# 如果没达到百分百,那么继续\n",
    "        \n",
    "            \n",
    "        return [learner_arr,f_layer,ind]\n",
    " \n",
    "    def predict(self, learner_arr, features):\n",
    "        \"\"\"\n",
    "        根据训练好的加法模型,对样本X进行分类\n",
    "        :param learner_arr: 训练好的加法模型\n",
    "        :param features: 待分类样本\n",
    "        :return: 分类结果\n",
    "        \"\"\"\n",
    "#         features = np.array(features)\n",
    "        if features.ndim == 1:  # 如果是1维的单个样本,则转换成二维的\n",
    "            features = features.reshape(1, features.shape[0])\n",
    "        fx = np.zeros((features.shape[0], 1))\n",
    "        fx_tmp = np.zeros((features.shape[0], 1))\n",
    "       \n",
    "        prediction=[]\n",
    "        ix=features.index\n",
    "        print(learner_arr)      \n",
    "#         print(features[['pr_test_files','commits']])\n",
    "#         rhs_thresh = 0.5 * sum(alpha_t)\n",
    "#         alphat_ht = alpha_t[t] * ht\n",
    "#         alphat_ht_sum = alphat_ht_sum + alphat_ht\n",
    "        for number in features.index:\n",
    "            flag=0\n",
    "            fx=[]\n",
    "#             base_learner=learner_arr[-1]\n",
    "            for base_learner in learner_arr:\n",
    "#                 print(\"base_learner===\",base_learner)\n",
    "\n",
    "                    gm = self.cal_gm(features.loc[[number]], base_learner['Feature'], base_learner['Value'], base_learner['ineq'])\n",
    "        #             print(\"gm====\",gm)\n",
    "                    fx_tmp=base_learner['Alpha'] * gm\n",
    "                    print(fx_tmp)\n",
    "                    if fx_tmp[0]<0:\n",
    "        #                     print(fx_tmp[0])\n",
    "                        prediction.append(-1)\n",
    "                        flag=1\n",
    "                        break\n",
    "                    else:\n",
    "                        fx += fx_tmp\n",
    "#                         prediction.append(1)\n",
    "#                 for x in fx_tmp:\n",
    "#                     if x<0:\n",
    "#                         prediction.append(-1)\n",
    "#                     else:\n",
    "#                         fx += fx_tmp\n",
    "#             if flag==0:\n",
    "#                 for x in fx:\n",
    "#                     print(fx)\n",
    "#                     prediction.append(1) \n",
    "#     if x>=0 else  prediction.append(-1) \n",
    "        return prediction\n",
    "\n",
    "    \n",
    "def test():\n",
    "    from sklearn.datasets import load_iris\n",
    "    import math\n",
    "    from collections import Counter\n",
    "    from sklearn.model_selection import train_test_split,ShuffleSplit,StratifiedKFold,cross_val_score\n",
    "#     iris = load_iris()\n",
    "#     X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "#     y = pd.DataFrame(iris.target,columns=['label'])\n",
    "#     new_data=pd.concat([X,y],axis=1)\n",
    "#     one_train=new_data[new_data.label==0].head(20)\n",
    "#     two_train=new_data[new_data.label==1].head(60)\n",
    "#     train_data=pd.concat([one_train,two_train],axis=0,ignore_index=True)\n",
    "#     from sklearn.utils import shuffle\n",
    "#     train_data=shuffle(train_data)\n",
    "#     y_data=train_data['label']\n",
    "#     x_data=train_data.drop(['label'],axis=1)\n",
    "#     print(x_data.shape[0])\n",
    "#     x_train,x_test,y_train,y_test=train_test_split(x_data,y_data,test_size=0.3,random_state=5)\n",
    "#     from sklearn.ensemble import IsolationForest\n",
    " \n",
    "#     y_test[y_test[:] == 0]=-1\n",
    "#     iForest= IsolationForest(n_estimators=20)\n",
    "\n",
    "#     iForest=iForest.fit(x_train)\n",
    "\n",
    "#     pred = iForest.predict(x_test)\n",
    "\n",
    "#     print(pred)\n",
    "#     print(y_test)\n",
    "    \n",
    "# #     print(features)\n",
    "# #     print(target)\n",
    "# #     print(target[:])\n",
    "    new_data=pd.read_csv('./structr/new_metric/gh_team_size/DSpace@DSpace_newmerge.csv',low_memory=False)\n",
    "    b=new_data[['pr_status','last_label','now_label','id','now_build_id']]\n",
    "    new_data=new_data.drop(['pr_status','last_label','now_label','id','now_build_id'],axis=1)\n",
    "    ix=new_data.index\n",
    "#             new_data=new_data.drop(['import_gum','import_change_count'],axis=1)\n",
    "    feature_names = list(new_data.columns.values)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a= min_max_scaler.fit_transform(new_data)\n",
    "    aa=pd.DataFrame(a,columns=feature_names,index=ix)\n",
    "    new_data=pd.concat([aa,b],axis=1)\n",
    "    new_data=new_data.drop(['now_duration'],axis=1)\n",
    "    new_data_fail=new_data[~new_data['last_label'].isin([\"1\"])]\n",
    "    test_size=math.ceil(new_data_fail.shape[0]/5)\n",
    "    \n",
    "    new_data_ix=new_data.index\n",
    "    test_data_begin=new_data_fail.tail(test_size)\n",
    "   \n",
    "    ix=test_data_begin.index\n",
    "   \n",
    "\n",
    "    test_data=new_data.iloc[ix[0]:new_data_ix[-1]+1,:]\n",
    "  \n",
    "    ix_test=test_data.index\n",
    "    train_data=new_data.iloc[0:ix_test[0],:]\n",
    "    train_data=train_data.drop(['id','now_build_id'],axis=1)\n",
    "    test_data=test_data.drop(['id','now_build_id'],axis=1)\n",
    "    y_train=train_data['now_label']\n",
    "    \n",
    "    x_train=train_data.drop(['now_label'],axis=1)\n",
    "    y_test=test_data['now_label']  \n",
    "    x_test=test_data.drop(['now_label'],axis=1)\n",
    "    \n",
    "    y_train[y_train[:] == 0] = -1\n",
    "    y_test[y_test[:] == 0]=-1\n",
    "    \n",
    "#     target_new = np.array(y_train).reshape(y_train.shape[0], 1)\n",
    "    ada = Adaboost(n_learners=30,limit=5)\n",
    "    print(y_train.shape[0])\n",
    "    learner_arr = ada.cal_cascade(x_train, y_train)\n",
    "    predicton = ada.predict(learner_arr, x_train)\n",
    "#     iForest= IsolationForest(n_estimators=20,contamination=0.25)\n",
    "\n",
    "#     iForest=iForest.fit(x_train)\n",
    "\n",
    "#     predicton = iForest.predict(x_test)\n",
    "\n",
    "    \n",
    "    from sklearn.metrics.scorer import make_scorer,f1_score,recall_score,precision_score\n",
    "    print(len(predicton))\n",
    "    print(( y_train))\n",
    "    correct = [1 if a == b else 0 for a, b in zip(predicton, y_train)]\n",
    "    print(f1_score( y_train,predicton,average=None)[0])\n",
    "    print(f1_score( y_train,predicton,average='weighted'))\n",
    "    print(correct.count(1) / len(correct))\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "[1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "a=[[1,2,3],[2,3,4]]\n",
    "print(a[-1])\n",
    "prediction=[]\n",
    "for x in a[-1]:\n",
    "    prediction.append(1) if x>=0 else  prediction.append(-1) \n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a  b  c  d  target\n",
      "0  1  3  4  5       1\n",
      "1  1  2  3  4       0\n",
      "2  2  2  3  0       1\n",
      "3  2  2  3  4       1\n",
      "4  2  3  4  1       1\n",
      "5  1  4  5  6       0\n",
      "1    True\n",
      "Name: a, dtype: bool\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'weight' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3732537becfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# print(weight)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# weight=np.array(weight).reshape(len(weight), 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m# gm[features['a'] <= 1,0] = 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# print(gm)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weight' is not defined"
     ]
    }
   ],
   "source": [
    "a=[[1,3,4,5,1],[1,2,3,4,0],[2,2,3,0,1],[2,2,3,4,1],[2,3,4,1,1],[1,4,5,6,0]]\n",
    "a=pd.DataFrame(a,columns=['a','b','c','d','target'])\n",
    "print(a)\n",
    "b=a.loc[[1]]\n",
    "print(b['a']==1)\n",
    "# target =a['target']\n",
    "# print(target)\n",
    "# target[target[:] == 0] = -1\n",
    "# print(target)\n",
    "# print(np.count_nonzero(target==-1))\n",
    "# print(np.count_nonzero(target==1))\n",
    "# features = a.drop(['target'],axis=1)\n",
    "# negNum=np.count_nonzero(target==1)\n",
    "# posNum=np.count_nonzero(target==-1)\n",
    "# gm = np.ones((features.shape[0], 1))\n",
    "\n",
    "# weight = [1.0/(2*negNum) if i==1 else 1.0/(2*posNum)  for i in target]\n",
    "# print(weight)\n",
    "# weight=np.array(weight).reshape(len(weight), 1)\n",
    "print(weight)\n",
    "# gm[features['a'] <= 1,0] = 0\n",
    "# print(gm)\n",
    "# recall_weight=[]\n",
    "# ind=[]\n",
    "# print(target)\n",
    "\n",
    "# print(np.dot(gm.T, gm)[0,0])\n",
    "# # print(np.log((1 - 0.2) / max(0.2, 1e-16)) / wrong\n",
    "# for i in range(2, 5):\n",
    "#       print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=['24305', '24379', '24375', '24306', '195387', '24309', '24339', '24344', '24284', '24299', '24296', '\\\\N', '24350', '24320', '24298', '24363', '24331', '24268', '24312', '24280', '24397', '24304', '24301', '24277', '24325', '24326', '24334', '24317', '24286', '24283', '24285', '24267', '24272', '24262', '24265', '24282', '24276', '24275', '24254', '24279', '24261', '24329', '24314', '24245', '24239', '24403', '24359', '24242', '24291', '195417', '195393', '24269', '24244', '24243', '24248', '24237', '\\\\N', '24230', '24252', '24226', '24302', '24323', '24216', '24307', '24263', '24270', '24235', '24251', '24228', '24231', '24257', '24232', '24208', '24258', '24210', '24206', '24213', '24259', '24214', '24219', '24236', '24211', '24229', '24202', '24201', '24196', '\\\\N', '24198', '24221', '195360', '24215', '24200', '24197', '24193', '195296', '24212', '24303', '24294', '24203', '24250', '24218', '24234', '24249', '24278', '24288', '24195', '24186', '24204', '\\\\N', '24194', '24190', '24260', '24225', '\\\\N', '24187', '24179', '24171', '24189', '24199', '24188', '24174', '24165', '24180', '24192', '\\\\N', '24295', '195369', '24238', '24172', '24185', '195327', '24207', '24209', '24222', '24255', '24164', '195354', '24175', '24183', '195389', '24159', '24158', '195286', '195383', '24292', '24253', '24241', '24246', '24699', '24177', '24156', '24142', '24181', '24154', '\\\\N', '24160', '24147', '24167', '24155', '24141', '24170', '\\\\N', '24166', '24145', '24146', '24157', '24144', '24151', '24148', '24161', '24168', '24133', '24119', '24122', '24136', '24139', '24129', '24128', '24134', '24121', '24125', '24124', '24152', '24131', '24138', '\\\\N', '24126', '24118', '24115', '24123', '24116', '24271', '24273', '24113', '\\\\N', '24220', '24233', '24109', '\\\\N', '24224', '24223', '24227', '24114', '24117', '24104', '24137', '24108', '24110', '24111', '24098', '24112', '24092', '195357', '24127', '24084', '24099', '24090', '24120', '24130', '24097', '195353', '24140', '24150', '24080', '24103', '24081', '24205', '195403', '195397', '195358', '24094', '24086', '24082', '24102', '24085', '24088', '24095', '24149', '24143', '24093', '24089', '24105', '24107', '24191', '24076', '24054', '24132', '24135', '195313', '24079', '24062', '24067', '24066', '24068', '24053', '24058', '24091', '24055', '24057', '24042', '24038', '24096', '24087', '195304', '24060', '24071', '\\\\N', '24051', '24041', '24043', '24030', '24031', '24073', '24077', '24037', '24059', '24032', '24021', '24045', '24027', '24083', '24178', '24069', '24072', '24046', '24025', '24006', '24078', '24022', '195418', '24070', '195409', '24039', '24040', '195331', '24009', '23990', '23987', '24008', '24013', '\\\\N', '23998', '\\\\N', '23994', '\\\\N', '23986', '24002', '23985', '24014', '23988', '24106', '23989', '24000', '23995', '24075', '24044', '24010', '24012', '24023', '24016', '23996', '24001', '24019', '23997', '24017']\n",
    "a=[24379, 24289, 24306, 195387, 24309, 24339, 24344, 24293, 24284, 24299, 24296, 24320, 24350, 24274, 24298, 24331, 24363, 24266, 24268, 24280, 24312, 24281, 24397, 24301, 24304, 24277, 24325, 24326, 24317, 24334, 24310, 24286, 24283, 24285, 24287, 24267, 24272, 24262, 24265, 24282, 24276, 24275, 24254, 24279, 24261, 24264, 24314, 24329, 24256, 24245, 24239, 24359, 24403, 24240, 24242, 24291, 195417, 195393, 24269, 24244, 24243, 24248, 24237, 24230, 24252, 24226, 24302, 24323, 24216, 24263, 24307, 24270, 24247, 24235, 24251, 24228, 24231, 24257, 24232, 24208, 24258, 24210, 24206, 24213, 24259, 24214, 24219, 24236, 24211, 24229, 24202, 24201, 24196, 24198, 24221, 195360, 24215, 24200, 24197, 24193, 195296, 24212, 24294, 24303, 24203, 24218, 24250, 24234, 24249, 24278, 24288, 24182, 24195, 24186, 24204, 24194, 24190, 24225, 24260, 24184, 24187, 24179, 24171, 24189, 24199, 24188, 24174, 24165, 24180, 24192, 24295, 195369, 24238, 24172, 24185, 195327, 24207, 24209, 24222, 24255, 24173, 24164, 195354, 24175, 24183, 195389, 24159, 24158, 195286, 195383, 24253, 24292, 24169, 24241, 24246, 24162, 24699, 24177, 24156, 24142, 24181, 24154, 24160, 24147, 24167, 24155, 24141, 24170, 24166, 24145, 24146, 24157, 24144, 24151, 24148, 24161, 24168, 24153, 24133, 24119, 24122, 24136, 24139, 24129, 24128, 24134, 24121, 24125, 24124, 24152, 24131, 24138, 24126, 24118, 24115, 24123, 24116, 24271, 24273, 24113, 24220, 24233, 24109, 24224, 24223, 24227, 24114, 24117, 24104, 24137, 24108, 24110, 24111, 24098, 24112, 24092, 195357, 24127, 24084, 24099, 24090, 24120, 24130, 24097, 195353, 24140, 24150, 24080, 24103, 24081, 24100, 24205, 195403, 195397, 195358, 24094, 24086, 24082, 24102, 24085, 24088, 24095, 24143, 24149, 24065, 24093, 24089, 24105, 24107, 24191, 24076, 24054, 24074, 24132, 24135, 195313, 24079, 24062, 24067, 24066, 24068, 24061, 24053, 24058, 24091, 24055, 24057, 24042, 24038, 24087, 24096, 195304, 24036, 24060, 24071, 24051, 24041, 24043, 24030, 24031, 24073, 24077, 24037, 24059, 24032, 24021, 24045, 24027, 24083, 24178, 24069, 24072, 24047, 24046, 24025, 24006, 24078, 24022, 195418, 24070, 195409, 24039, 24040, 195331, 24024, 24009, 23990, 23987, 24008, 24013, 23998, 23994, 23986, 24002, 23985, 24014, 23988, 24106, 23989, 23980, 23995, 24000, 24044, 24075, 24010, 24012, 24005, 24016, 24023, 23996, 24001, 24019, 23997, 24017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1 0\n",
      "2\n",
      "2 1\n",
      "3\n",
      "3 2\n",
      "[0, 0.19999999999999998, -0.04999999999999999, 0.44999999999999996]\n",
      "   a  b  c  d  fail_rate_diff  rate\n",
      "0  1  2  3  4            0.00  0.10\n",
      "1  2  2  3  0            0.20  0.30\n",
      "2  2  3  4  1           -0.05  0.25\n",
      "3  1  4  5  6            0.45  0.70\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "# shapes=a.shape[0]\n",
    "rate_diff=[]\n",
    "from decimal import Decimal\n",
    "ix=a.index\n",
    "indexs=list(a.index)\n",
    "shapes=len(indexs)\n",
    "for i in range(shapes):\n",
    "   \n",
    "    print(indexs[i])\n",
    "    if count<=shapes-1:\n",
    "        if count==0:\n",
    "            rate_diff.append(0)\n",
    "        else:\n",
    "            m=indexs[i]\n",
    "            n=indexs[i-1]\n",
    "            print(m,n)\n",
    "            rate_diff.append(a.loc[[m]]['rate'].values[0]- a.loc[[n]]['rate'].values[0])\n",
    "    count+=1\n",
    "print(rate_diff)\n",
    "rate_diff=pd.Series(rate_diff,index=ix)\n",
    "#     rate_diff=pd.DataFrame(rate_diff,index=ix,columns=['fail_rate_diff'])\n",
    "a.insert(4,'fail_rate_diff',rate_diff)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [1, 23, 5]}\n"
     ]
    }
   ],
   "source": [
    "dic={}\n",
    "dic[1]=[1,23,5]\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [1, 23, 5], 23: [1, 34, 5]}\n"
     ]
    }
   ],
   "source": [
    "dic[23]=[1,34,5]\n",
    "print(dic)\n",
    "dic[23].append(88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "tmp_build_number,tmp_file_modified,tmp_file_added,tmp_file_deleted,tmp_line_added,tmp_line_deleted=0,1,0,0,0,0\n",
    "print(tmp_file_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.75\n"
     ]
    }
   ],
   "source": [
    "ratios=[[6, 3],[4, 2]]\n",
    "        #, [18,9], [4, 2]]\n",
    "print(np.mean(ratios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
